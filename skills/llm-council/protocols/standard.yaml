# LLM Council - Standard Protocol Manifest v4.4
# Manifest format: indexes prompts and rubrics, auto-detects evaluation criteria

version: "4.4"

metadata:
  name: standard-council
  description: |
    Multi-LLM deliberation protocol with mandatory cross-evaluation.
    Dynamically discovers available LLMs - no predefined roles.

# ============================================================
# RESOURCES
# ============================================================

resources:
  prompts:
    stage0.5_select: prompts/stage0.5_select.md  # NEW: Smart rubric selection
    stage1_collect: prompts/stage1_collect.md
    stage2_evaluate: prompts/stage2_evaluate.md
    stage3_synthesize: prompts/stage3_synthesize.md

  rubrics:
    default: null  # Use SKILL.md embedded default
    domains:
      # Original 3
      code-review: rubrics/code-review.yaml
      factual-qa: rubrics/factual-qa.yaml
      technical-decision: rubrics/technical-decision.yaml
      # New 10
      debugging: rubrics/debugging.yaml
      summarization: rubrics/summarization.yaml
      creative-writing: rubrics/creative-writing.yaml
      brainstorming: rubrics/brainstorming.yaml
      translation: rubrics/translation.yaml
      instructional: rubrics/instructional.yaml
      information-extraction: rubrics/information-extraction.yaml
      project-planning: rubrics/project-planning.yaml
      customer-support: rubrics/customer-support.yaml
      safety-critical: rubrics/safety-critical.yaml

# ============================================================
# RUBRIC SELECTION (v4.4: Smart Selection + Dynamic Weights)
# ============================================================

rubric_selection:
  mode: smart  # smart (Host LLM analyzes) replaces keyword matching
  prompt_template: prompts/stage0.5_select.md

  # Weight constraints for dynamic adjustment
  weight_constraints:
    # Tier 1: Truth-Anchors (strict protection)
    tier1:
      accuracy: { min: 15, max: 50 }
      verifiability: { min: 8, max: 35 }

    # Tier 2: Expression-Attributes (more flexible)
    tier2:
      completeness: { min: 8, max: 40 }
      clarity: { min: 5, max: 35 }
      actionability: { min: 5, max: 45 }
      relevance: { min: 5, max: 25 }

    # Combined constraints
    combined:
      truth_anchor_sum: { min: 30 }  # accuracy + verifiability >= 30%

    total: 100

  # Dual score output
  scoring:
    core_score:
      description: "Fixed Core6 weights for cross-task comparison"
      weights: { accuracy: 30, verifiability: 15, completeness: 20, clarity: 15, actionability: 10, relevance: 10 }
    overall_score:
      description: "Dynamic weights optimized for specific task"
      weights: dynamic  # Uses final_weights from Stage 0.5

# ============================================================
# RUBRIC DETECTION (Legacy - kept for reference)
# ============================================================

rubric_detection:
  code-review:
    keywords:
      - "review"
      - "code"
      - "PR"
      - "pull request"
      - "refactor"
      - "bug"
      - "fix"
    patterns:
      - "review (this|my|the) (code|PR|pull request)"
      - "(what|how).*(wrong|improve|better).*code"

  factual-qa:
    keywords:
      - "what is"
      - "who is"
      - "when did"
      - "where is"
      - "how many"
      - "explain"
      - "define"
    patterns:
      - "^(what|who|when|where|why|how)\\b"
      - "(explain|define|describe).*\\?"

  technical-decision:
    keywords:
      - "should I"
      - "which is better"
      - "recommend"
      - "choose"
      - "decision"
      - "tradeoff"
      - "compare"
    patterns:
      - "(should|would).*(use|choose|pick|recommend)"
      - "(which|what).*(better|best|prefer)"
      - "(pros|cons|tradeoff|comparison)"

  debugging:
    priority: high  # Higher priority than code-review for error-related queries
    keywords:
      - "fix"
      - "error"
      - "exception"
      - "crash"
      - "debug"
      - "not working"
      - "failed"
      - "stack trace"
      - "troubleshoot"

  summarization:
    keywords:
      - "summarize"
      - "summary"
      - "tl;dr"
      - "recap"
      - "key takeaways"
      - "meeting notes"
      - "digest"

  creative-writing:
    keywords:
      - "story"
      - "poem"
      - "fiction"
      - "narrative"
      - "roleplay"
      - "script"
      - "creative"
      - "novel"

  brainstorming:
    keywords:
      - "brainstorm"
      - "ideas"
      - "suggest"
      - "alternatives"
      - "generate"
      - "options"
      - "what if"

  translation:
    keywords:
      - "translate"
      - "translation"
      - "english to"
      - "chinese to"
      - "localization"

  instructional:
    keywords:
      - "how to"
      - "tutorial"
      - "guide"
      - "step by step"
      - "teach me"
      - "walk me through"

  information-extraction:
    keywords:
      - "extract"
      - "parse"
      - "to json"
      - "to csv"
      - "structured"
      - "schema"
      - "convert to"

  project-planning:
    keywords:
      - "plan"
      - "roadmap"
      - "milestone"
      - "timeline"
      - "schedule"
      - "phases"
      - "sprint"

  customer-support:
    keywords:
      - "support"
      - "customer"
      - "ticket"
      - "complaint"
      - "apologize"
      - "help desk"

  safety-critical:
    priority: highest  # Always check first for safety-related queries
    keywords:
      - "medical"
      - "medication"
      - "dosage"
      - "diagnosis"
      - "legal"
      - "law"
      - "contract"
      - "tax"
      - "investment"
      - "financial advice"

# ============================================================
# PREREQUISITES
# ============================================================

prerequisites:
  external_llm_required: true
  min_participants: 2
  discovery: dynamic
  description: |
    Discovers available LLM tools at runtime.
    All participants are equal - no predefined roles.

# ============================================================
# EXECUTION
# ============================================================

execution:
  mode_priority:
    - multi_agent
    - parallel_tools
  timeout:
    per_llm_call: 120
    total: 300

# ============================================================
# CROSS-EVALUATION
# ============================================================

cross_evaluation:
  mandatory: true
  self_evaluation: false
  anonymization:
    shuffle_order: true
    relabel_responses: true
    strip_metadata: true
    record_mapping: true
    reveal_timing: after_all_scores

# ============================================================
# SECURITY
# ============================================================

security:
  treat_outputs_as_untrusted: true
  never_execute_instructions: true
