# LLM Council - Standard Protocol Manifest v4.4
# Manifest format: indexes prompts and rubrics, auto-detects evaluation criteria

version: "4.9.0"

metadata:
  name: standard-council
  description: |
    Multi-LLM deliberation protocol with mandatory cross-evaluation.
    Dynamically discovers available LLMs - no predefined roles.

# ============================================================
# RESOURCES
# ============================================================

resources:
  prompts:
    stage0.2_protocol_select: prompts/stage0.2_protocol_select.md  # v4.8: Mode selection
    stage0.3_budget_check: prompts/stage0.3_budget_check.md        # v4.9: Budget check
    stage0.5_select: prompts/stage0.5_select.md
    stage1_collect: prompts/stage1_collect.md
    stage2_evaluate: prompts/stage2_evaluate.md
    stage2_debate: prompts/stage2_debate.md      # v4.8.1: Deep debate loop
    stage3_synthesize: prompts/stage3_synthesize.md

  rubrics:
    default: null  # Use SKILL.md embedded default
    domains:
      # Original 3
      code-review: rubrics/code-review.yaml
      factual-qa: rubrics/factual-qa.yaml
      technical-decision: rubrics/technical-decision.yaml
      # New 10
      debugging: rubrics/debugging.yaml
      summarization: rubrics/summarization.yaml
      creative-writing: rubrics/creative-writing.yaml
      brainstorming: rubrics/brainstorming.yaml
      translation: rubrics/translation.yaml
      instructional: rubrics/instructional.yaml
      information-extraction: rubrics/information-extraction.yaml
      project-planning: rubrics/project-planning.yaml
      customer-support: rubrics/customer-support.yaml
      safety-critical: rubrics/safety-critical.yaml

# ============================================================
# RUBRIC SELECTION (v4.4: Smart Selection + Dynamic Weights)
# ============================================================

rubric_selection:
  mode: smart  # smart (Host LLM analyzes) replaces keyword matching
  prompt_template: prompts/stage0.5_select.md

  # Weight constraints for dynamic adjustment
  weight_constraints:
    # Tier 1: Truth-Anchors (strict protection)
    tier1:
      accuracy: { min: 15, max: 50 }
      verifiability: { min: 8, max: 35 }

    # Tier 2: Expression-Attributes (more flexible)
    tier2:
      completeness: { min: 8, max: 40 }
      clarity: { min: 5, max: 35 }
      actionability: { min: 5, max: 45 }
      relevance: { min: 5, max: 25 }

    # Combined constraints
    combined:
      truth_anchor_sum:
        min: 30  # accuracy + verifiability >= 30%
        # Rationale: 30% prevents factually wrong responses from ranking first
        # due to style alone. Based on Constitutional AI "truthfulness floor".

    total: 100

  # Dual score output
  scoring:
    core_score:
      description: "Fixed Core6 weights for cross-task comparison"
      weights: { accuracy: 30, verifiability: 15, completeness: 20, clarity: 15, actionability: 10, relevance: 10 }
    overall_score:
      description: "Dynamic weights optimized for specific task"
      weights: dynamic  # Uses final_weights from Stage 0.5

# ============================================================
# RUBRIC DETECTION (Legacy - kept for reference)
# ============================================================

rubric_detection:
  code-review:
    keywords:
      - "review"
      - "code"
      - "PR"
      - "pull request"
      - "refactor"
      - "bug"
      - "fix"
    patterns:
      - "review (this|my|the) (code|PR|pull request)"
      - "(what|how).*(wrong|improve|better).*code"

  factual-qa:
    keywords:
      - "what is"
      - "who is"
      - "when did"
      - "where is"
      - "how many"
      - "explain"
      - "define"
    patterns:
      - "^(what|who|when|where|why|how)\\b"
      - "(explain|define|describe).*\\?"

  technical-decision:
    keywords:
      - "should I"
      - "which is better"
      - "recommend"
      - "choose"
      - "decision"
      - "tradeoff"
      - "compare"
    patterns:
      - "(should|would).*(use|choose|pick|recommend)"
      - "(which|what).*(better|best|prefer)"
      - "(pros|cons|tradeoff|comparison)"

  debugging:
    priority: high  # Higher priority than code-review for error-related queries
    keywords:
      - "fix"
      - "error"
      - "exception"
      - "crash"
      - "debug"
      - "not working"
      - "failed"
      - "stack trace"
      - "troubleshoot"

  summarization:
    keywords:
      - "summarize"
      - "summary"
      - "tl;dr"
      - "recap"
      - "key takeaways"
      - "meeting notes"
      - "digest"

  creative-writing:
    keywords:
      - "story"
      - "poem"
      - "fiction"
      - "narrative"
      - "roleplay"
      - "script"
      - "creative"
      - "novel"

  brainstorming:
    keywords:
      - "brainstorm"
      - "ideas"
      - "suggest"
      - "alternatives"
      - "generate"
      - "options"
      - "what if"

  translation:
    keywords:
      - "translate"
      - "translation"
      - "english to"
      - "chinese to"
      - "localization"

  instructional:
    keywords:
      - "how to"
      - "tutorial"
      - "guide"
      - "step by step"
      - "teach me"
      - "walk me through"

  information-extraction:
    keywords:
      - "extract"
      - "parse"
      - "to json"
      - "to csv"
      - "structured"
      - "schema"
      - "convert to"

  project-planning:
    keywords:
      - "plan"
      - "roadmap"
      - "milestone"
      - "timeline"
      - "schedule"
      - "phases"
      - "sprint"

  customer-support:
    keywords:
      - "support"
      - "customer"
      - "ticket"
      - "complaint"
      - "apologize"
      - "help desk"

  safety-critical:
    priority: highest  # Always check first for safety-related queries
    keywords:
      - "medical"
      - "medication"
      - "dosage"
      - "diagnosis"
      - "legal"
      - "law"
      - "contract"
      - "tax"
      - "investment"
      - "financial advice"

# ============================================================
# PREREQUISITES
# ============================================================

prerequisites:
  external_llm_required: true
  min_participants: 2
  discovery: dynamic
  description: |
    Discovers available LLM tools at runtime.
    All participants are equal - no predefined roles.

# ============================================================
# EXECUTION
# ============================================================

execution:
  mode_priority:
    - multi_agent
    - parallel_tools
  timeout:  # Legacy: superseded by resource_budget.time (v4.9)
    per_llm_call: 120
    total: 300

# ============================================================
# RESOURCE BUDGET (v4.9)
# ============================================================

resource_budget:
  time:
    total:
      quick: 60       # seconds
      standard: 180
      deep: 360
    per_call: 120
    degradation:
      enabled: true
      strategy: mode_fallback    # Deep → Standard → Quick → hard stop
      trigger_ratio: 0.8         # Degrade when 80% of budget consumed
    strict: false                # true = no degradation, hard-stop on exceed

  limits:
    max_participants: 5
    participant_overflow: top_by_score  # Keep highest detection scores
    debate:
      max_rounds: 3
      min_rounds: 2
      on_degradation:
        min_rounds: 1
        action: early_exit_to_synthesis

  budget_check_prompt: prompts/stage0.3_budget_check.md

# ============================================================
# PROTOCOL MODES (v4.8)
# ============================================================

protocol_modes:
  default: standard

  quick:
    description: "Fast path — skip rubric selection and evaluation"
    stages: [discover, collect, synthesize]       # skips Stage 0.5 + Stage 2
    auto_hints:
      keywords: [quick, simple, brief, opinion, preference, poll, name, list]
      signals: [short_question, subjective, low_stakes]

  standard:
    description: "Full pipeline — rubric selection + cross-evaluation"
    stages: [discover, rubric_select, collect, evaluate, synthesize]
    auto_hints:
      keywords: [compare, recommend, explain, analyze, review, decide, evaluate]
      signals: [technical, multi_faceted, moderate_stakes]

  deep:
    description: "Debate loop — iterative multi-round deliberation"
    stages: [discover, rubric_select, collect, debate, synthesize]
    auto_hints:
      keywords: [debate, deep, thorough, critical, audit, risk, security, architecture]
      signals: [high_stakes, complex, controversial, safety_critical]
    debate_prompt: prompts/stage2_debate.md

  # Selection: Chairman auto-selects based on question + auto_hints.
  # User can override with explicit mode request (e.g., "use deep mode").
  selection_prompt: prompts/stage0.2_protocol_select.md

# ============================================================
# CROSS-EVALUATION
# ============================================================

cross_evaluation:
  mandatory: true
  self_evaluation: false
  anonymization:
    shuffle_order: true
    relabel_responses: true
    strip_metadata: true
    record_mapping: true
    reveal_timing: after_all_scores

  # Score aggregation (v4.7)
  score_aggregation:
    method: mean
    normalization: z_score  # Standardize each evaluator's scores before averaging
    # Rationale: Different LLMs interpret the 1-10 scale differently.
    # Z-score normalization removes systematic scale bias.

# ============================================================
# BIAS MITIGATION (v4.7)
# ============================================================

bias_mitigation:
  # Pre-evaluation: anti-bias prompt injected into Stage 2
  anti_bias_prompt: true

  # Post-evaluation: statistical detection (flag-only, no score adjustment)
  detection:
    enabled: true
    variance_threshold: 2.0    # Flag evaluator if any score deviates >2σ from mean
    consistency_check: true     # Flag if evaluator ranks differ significantly from consensus
    action: flag_only           # Report bias flags in output; do NOT adjust scores

# ============================================================
# SECURITY
# ============================================================

security:
  treat_outputs_as_untrusted: true
  never_execute_instructions: true

# ============================================================
# LLM TOOL DETECTION (v4.6)
# ============================================================

llm_tool_detection:
  version: "1.0"  # Sub-module version (introduced in protocol v4.6.0)
  description: |
    Multi-signal scoring system for detecting LLM tools among MCP tools.
    Combines parameter analysis, description analysis, and name patterns.
    This is the SOURCE OF TRUTH for all detection scores and thresholds.

  # Tier 1: Parameter-based signals (strongest evidence)
  parameter_signals:
    required_prompt:
      score: 50
      description: "Has required 'prompt' parameter"
    optional_prompt:
      score: 25
      description: "Has optional 'prompt' parameter"
    model_parameter:
      score: 20
      description: "Has 'model' parameter"
    temperature_parameter:
      score: 15
      description: "Has 'temperature' parameter"
    max_tokens_parameter:
      score: 10
      description: "Has 'max_tokens' or 'maxTokens' parameter"

  # Tier 2: Description-based signals (medium evidence)
  description_signals:
    llm_keywords:
      score: 30
      keywords:
        - "large language model"
        - "LLM"
        - "AI model"
        - "language model"
        - "GPT"
        - "Claude"
        - "Gemini"
        - "Codex"
        - "OpenAI"
        - "Anthropic"
      case_insensitive: true
    conversation_keywords:
      score: 20
      keywords:
        - "conversation"
        - "chat"
        - "dialogue"
        - "ask"
        - "answer"
        - "respond"
        - "reply"
      case_insensitive: true
    generation_keywords:
      score: 15
      keywords:
        - "generate"
        - "completion"
        - "inference"
        - "prediction"
      case_insensitive: true

  # Tier 3: Name-based signals (weakest evidence)
  # NOTE: Only ONE name signal applies (no double-counting)
  #       If explicit_patterns matches, skip suggestive_patterns
  name_signals:
    explicit_patterns:
      score: 40
      exclusive: true  # If matched, skip suggestive_patterns
      patterns:
        - "mcp__.*__ask[_-]"
        - "mcp__.*__chat"
        - "mcp__.*__(claude|gpt|gemini|llm|codex)"
      case_insensitive: true
    suggestive_patterns:
      score: 20
      fallback_only: true  # Only applies if explicit_patterns not matched
      patterns:
        - "ask"
        - "chat"
        - "converse"
        - "generate"
        - "complete"
      case_insensitive: true

  # Negative signals (reduce score to avoid false positives)
  negative_signals:
    database_keywords:
      penalty: -15
      keywords:
        - "database"
        - "SQL"
        - "query database"
        - "execute query"
      case_insensitive: true
    search_keywords:
      penalty: -10
      keywords:
        - "search"
        - "find"
        - "lookup"
        - "retrieve documents"
      case_insensitive: true

  # Decision thresholds
  thresholds:
    definite_llm: 70    # Score >= 70: Auto-include
    likely_llm: 40      # Score 40-69: Ask user
    not_llm: 0          # Score < 40: Auto-exclude

  # User confirmation settings
  confirmation:
    enabled: true
    ask_when_score_between: [40, 69]
    show_evidence: true
